{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gear Box Fault Detection\n",
    "\n",
    "Gearbox Fault Diagnosis Data set include the vibration dataset recorded by using SpectraQuestâ€™s Gearbox Fault Diagnostics Simulator. Dataset has been recorded with the help of 4 vibration sensors placed in four different direction. Data set has been recorded under variation of load from '0' to '90' percent. Data set has been recorded in two different scenario: \n",
    "\n",
    "1) Healthy condition and \n",
    "\n",
    "2) Broken Tooth Condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_list = []\n",
    "# for root, dirs, files in os.walk(\".\"):  \n",
    "#     for filename in files:\n",
    "#         name_list.append(filename)\n",
    "        \n",
    "# name_list.remove('Untitled.ipynb')\n",
    "# name_list.remove('.DS_Store')\n",
    "# name_list.remove('Untitled-checkpoint.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1  Load_data Function\n",
    "\n",
    "Load_data function will load a text file in the data folder and will open as a Pandas DataFrame. The file can be in '.txt' or '.csv' extention file. This function developed to be used in the Join_table function. Input arguments for the function is \n",
    "\n",
    "- file_path = file name without the location of the file. Keep the files in the same directory of the python file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Load_data(file_path):\n",
    "    \n",
    "    path = file_path.lower().split(\".\")\n",
    "    \n",
    "    if path[1] == \"csv\":\n",
    "        freq = int(path[0][1:3])\n",
    "        load = int(path[0][5:])\n",
    "        \n",
    "        df = pd.read_csv(file_path, header= None, sep = \",\")\n",
    "        if df.shape[1] == 5:\n",
    "            df.columns = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "            \n",
    "        df = df.drop([\"S5\"], axis = 1)\n",
    "        df[\"Freq\"] = [freq]*df.shape[0]\n",
    "        df[\"Load_percent\"] = [load]*df.shape[0]\n",
    "        if path[0] == \"b\":\n",
    "            df[\"Fault\"] = [1]*df.shape[0]\n",
    "        elif path[0] == \"h\":\n",
    "             df[\"Fault\"] = [0]*df.shape[0]\n",
    "    \n",
    "    elif path[1] == \"txt\":\n",
    "        freq = int(path[0][1:3])\n",
    "        load = int(path[0][5:])\n",
    "        \n",
    "        df = pd.read_csv(file_path, header= None, sep = \"\\t\")\n",
    "        df.columns = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "        df = df.drop([\"S5\"], axis = 1)\n",
    "        df[\"Freq\"] = [freq]*df.shape[0]\n",
    "        df[\"Load_percent\"] = [load]*df.shape[0]\n",
    "        if path[0][0] == \"b\":\n",
    "            df[\"Fault\"] = [1]*df.shape[0]\n",
    "        elif path[0][0] == \"h\":\n",
    "             df[\"Fault\"] = [0]*df.shape[0]\n",
    "        \n",
    "    else:\n",
    "        print(\"Invald format of file. Make sure file is either csv or txt.\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Join_table funciton\n",
    "\n",
    "Join_table function will load all the files present in the file list given as an argument and will concat them all as a pandas Dataframe. The only argument for this function is as follows :- \n",
    "\n",
    "- file_list = list of all the files to be loaded. All files extentions must be either \".txt\" or \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Join_table(file_list):\n",
    "    \n",
    "    df = Load_data(file_list[0])\n",
    "    \n",
    "    for file in file_list[1:]:\n",
    "        data = Load_data(file)\n",
    "        df = pd.concat([df, data], axis = 0)\n",
    "\n",
    "    df = df.reset_index().drop([\"index\"], axis = 1)\n",
    "        \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Trimming function\n",
    "\n",
    "Dataset consist of 10 files of faulty gearbox data and healty gearbox data each at different load percentage. Each of these files has different number of data points beacuse of the different time length of experiment. Hence, to make each load percentage experiment data of same length, we need to bring all the dataset at the same dimension. Hence, this function will select first 87000 datapoints from each file. \n",
    "$$ 290 \\times 300 = 87000 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to trim each load to 87000 points. \n",
    "def Trimming(data):\n",
    "    \n",
    "    empty = data[:0]\n",
    "    for load in np.linspace(0,90,10):\n",
    "        dff = data[data[\"Load_percent\"] == load ]\n",
    "        df = dff[:87000]\n",
    "        empty = pd.concat([empty, df], axis = 0)\n",
    "    \n",
    "    return empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Feature Creation from frequency domain dataset\n",
    "\n",
    "The shape of the dataset after trimming is $(1740000, 7)$. The dataset shall be seperate on the basis of fault, i.e. \"Healthy\" or \"Broken\". From this data, we have to create feature which shall be used to train our machine learning algorithms to predict the type of fault. To create the features, Feature_create function is used. This function uses various sub functions, which are as follows: - \n",
    "\n",
    "1) FFT : - FFT function is to convert a signal from time domain to frequency domain. This function takes an pandas Dataframe as an input argument and spit out an absolute value of FFT in Pandas Dataframe format. \n",
    "\n",
    "2) Concate_features : -  This function is the sub-function of the RMS function. It is used to rearrange the the RMS values of frequency of each sensor and rename the columns as $ S*_F# $, where * is the sensor number and # is frequency number (ranging from 1 to 15 Hz). \n",
    "\n",
    "3) RMS : - This function is responsible to take the RMS values of all the datapoint lies in the frequency bin of a single frequency. In this setting, each bin contains 10 datapoints. \n",
    "\n",
    "\n",
    "Using above mentioned functions, Feature_create function create the features for the given dataset. \n",
    "As an output, you get 60 features, 15 for each sensors. \n",
    "\n",
    "The logic of the function is as follows:- \n",
    "\n",
    "- Separate the data on the basis of load_percentage. Output should have 87000 rows. \n",
    "\n",
    "- A subset of 300 datapoints from the separated dataset is used for the FFT. Thus likewise, we get 290 chunks of subset. \n",
    "\n",
    "- This subset of dataset is used for FFT. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.1\n",
    "def FFT(data):\n",
    "    col = data.columns.values\n",
    "    df_fft = np.fft.fft(data, axis = 0)\n",
    "    abs_df = pd.DataFrame(abs(df_fft), columns= col)\n",
    "    \n",
    "    return abs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.2\n",
    "def Concate_features(x):\n",
    "    index_list = [\"S\" + str(n) for n in np.arange(1,5)]\n",
    "    dff = pd.DataFrame({})\n",
    "    for index in index_list:\n",
    "\n",
    "        xx = x.loc[[index]]\n",
    "        col = [index + \"_\" + \"F\" + str(n) for n in np.arange(1,16)]\n",
    "        xx.columns = col \n",
    "        xx = xx.reset_index().drop([\"index\"], axis = 1)\n",
    "        dff = pd.concat([dff, xx], axis = 1)\n",
    "    return dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.3\n",
    "\n",
    "def RMS(data):\n",
    "    \n",
    "    col = [\"F\"+ str(n) for n in np.arange(1,16)]\n",
    "    \n",
    "    x = data[0:10]\n",
    "    x = x.applymap(lambda a : a**2)\n",
    "    x = pd.DataFrame((x.sum()/10),columns=[\"F1\"]).applymap(lambda a : math.sqrt(a))\n",
    "\n",
    "    for n in np.arange(1,15):\n",
    "        y = data[n*0:(n+1)*10]\n",
    "        y = y.applymap(lambda a : a**2)\n",
    "        y = pd.DataFrame((y.sum()/10) , columns= [col[n]]).applymap(lambda a : math.sqrt(a))\n",
    "        x = pd.concat([x, y], axis = 1)\n",
    "    \n",
    "    df = Concate_features(x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.0\n",
    "\n",
    "def Feature_create(data):\n",
    "    df = pd.DataFrame({})\n",
    "    for load in np.linspace(0,90,10):\n",
    "        dff = data[data[\"Load_percent\"] == load ]\n",
    "        X = dff[['S1', 'S2', 'S3', 'S4']]\n",
    "        for n in range(290):\n",
    "            time_domain = X[n*300:(n+1)*300]\n",
    "            freq_domain = FFT(time_domain)\n",
    "            f = freq_domain[0:150]\n",
    "            f = RMS(f)\n",
    "            \n",
    "            df = pd.concat([df, f], axis = 0 )\n",
    "            \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the file name list. The File_name.csv is the file with all the file names created separately \"\"\"\n",
    "\n",
    "name_list = pd.read_csv(\"File_name.csv\", header = None).T\n",
    "name_list = name_list.values\n",
    "name_list = list(name_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code 2 \"\"\"\n",
    "data = Join_table(file_list= name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Seperating the dataset on the basis of Fault, i.e. Healthy and broken.\"\"\"\n",
    "healthy = data[data[\"Fault\"] == 0]\n",
    "broken = data[data[\"Fault\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code 3\"\"\"\n",
    "df_health = Trimming(healthy)\n",
    "df_broken = Trimming(broken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code 4 \"\"\"\n",
    "\n",
    "health_data = Feature_create(df_health)\n",
    "health_data = health_data.reset_index().drop([\"index\"], axis=1) ## Reset the index inorder to concat.\n",
    "broken_data = Feature_create(df_broken)\n",
    "broken_data = broken_data.reset_index().drop([\"index\"], axis=1) ## Reset the index inorder to concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating Load_percentage data to join the main dataset.\"\"\"\n",
    "\n",
    "load_data = pd.DataFrame(pd.DataFrame([[n]*290 for n in np.arange(0,100,10)]).values.flatten(), columns= [\"Load\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Concating Load_percentage and Fault data with the healthy and broken dataset\"\"\"\n",
    "\n",
    "healthy = pd.concat([health_data, load_data], axis = 1)\n",
    "broken = pd.concat([broken_data, load_data], axis = 1)\n",
    "\n",
    "healthy[\"Fault\"] = [0]*healthy.shape[0]\n",
    "broken[\"Fault\"] = [1]*broken.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([healthy, broken], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(\"Processed_final_data.csv\", index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
